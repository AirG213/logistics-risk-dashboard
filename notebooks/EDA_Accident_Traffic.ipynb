{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61128e6",
   "metadata": {},
   "source": [
    "# üìä Analyse des accidents de la circulation aux √âtats-Unis (2016‚Äì2023)\n",
    "\n",
    "Ce notebook fait partie du pipeline **R&D R√©silience Logistique**.  \n",
    "Il se concentre sur le **module risque routier**, en explorant et nettoyant le dataset **US Accidents** pour fournir des indicateurs exploitables dans le tableau de bord final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf01c93",
   "metadata": {},
   "source": [
    "## üìÅ Structure du dataset\n",
    "\n",
    "| Variable             | Description courte                                                 | Type    | Exemples                    |\n",
    "| -------------------- | ------------------------------------------------------------------ | ------- | --------------------------- |\n",
    "| `ID`                 | Identifiant unique de l'accident                                   | object  | 'A-1', 'A-2'                |\n",
    "| `Source`             | Source de la donn√©e (ex: capteur, site partenaire)                 | object  | 'Source2'                   |\n",
    "| `Severity`           | Gravit√© de l'accident (1 √† 4)                                      | int     | 2, 3                        |\n",
    "| `Start_Time`         | Date et heure de d√©but de l'accident                               | datetime| '2016-02-08 05:46:00'       |\n",
    "| `End_Time`           | Date et heure de fin de l'accident                                 | datetime| '2016-02-08 11:00:00'       |\n",
    "| `Start_Lat`          | Latitude de d√©but                                                  | float   | 39.865147                   |\n",
    "| `Start_Lng`          | Longitude de d√©but                                                 | float   | -84.058723                  |\n",
    "| `End_Lat`            | Latitude de fin (souvent manquante)                                | float   | -                           |\n",
    "| `End_Lng`            | Longitude de fin (souvent manquante)                               | float   | -                           |\n",
    "| `Distance(mi)`       | Distance couverte par l'incident                                   | float   | 0.01                        |\n",
    "| `Description`        | Description textuelle de l'√©v√©nement                               | object  | 'Accident sur I-70 E...'    |\n",
    "| `Street`, `City`, `County`, `State`, `Zipcode`, `Country` | Localisation administrative                    | object  | 'Dayton', 'Montgomery', 'OH'|\n",
    "| `Timezone`           | Fuseau horaire local                                               | object  | 'US/Eastern'                |\n",
    "| `Airport_Code`       | Code de l'a√©roport le plus proche                                  | object  | 'KFFO'                      |\n",
    "| `Weather_Timestamp`  | Timestamp m√©t√©o associ√©                                            | datetime| '2016-02-08 05:58:00'       |\n",
    "| `Temperature(F)`     | Temp√©rature en Fahrenheit                                          | float   | 36.9                        |\n",
    "| `Wind_Chill(F)`      | Ressenti en Fahrenheit                                             | float   | 33.3                        |\n",
    "| `Humidity(%)`        | Taux d'humidit√©                                                    | float   | 91.0                        |\n",
    "| `Pressure(in)`       | Pression atmosph√©rique en pouces                                   | float   | 29.68                       |\n",
    "| `Visibility(mi)`     | Visibilit√© en miles                                                | float   | 10.0                        |\n",
    "| `Wind_Direction`     | Direction du vent                                                  | object  | 'Calm', 'SW'                |\n",
    "| `Wind_Speed(mph)`    | Vitesse du vent en mph                                             | float   | 3.5                         |\n",
    "| `Precipitation(in)`  | Pr√©cipitations en pouces                                           | float   | 0.02                        |\n",
    "| `Weather_Condition`  | Conditions m√©t√©o d√©crites                                          | object  | 'Light Rain', 'Overcast'    |\n",
    "| `Amenity` √† `Turning_Loop` | Divers indicateurs routiers bool√©ens (bump, stop, signal, etc.) | bool    | True / False                |\n",
    "| `Sunrise_Sunset` √† `Astronomical_Twilight` | Phase de la journ√©e                          | object  | 'Day', 'Night'              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3284a5f1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac37113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from meteostat import Stations, Daily, Hourly\n",
    "\n",
    "# R√©glages pandas & seaborn\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "print(\"Librairies charg√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872997af",
   "metadata": {},
   "source": [
    "# Chargement des donn√©es + premi√®res v√©rifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir le chemin du fichier\n",
    "file_path = \"../data/extracted/USA_Accidents_Traffic/US_Accidents_March23.csv\"\n",
    "\n",
    "# Chargement du CSV\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# V√©rifier dimensions et aper√ßu\n",
    "print(f\"Dimensions de df : {df.shape}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676042ae",
   "metadata": {},
   "source": [
    "# V√©rification d√©taill√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac379e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affiche infos g√©n√©rales : types, nulls\n",
    "df.info()\n",
    "\n",
    "# Statistiques descriptives pour colonnes num√©riques\n",
    "df.describe().T\n",
    "\n",
    "# Liste compl√®te des colonnes pour pr√©parer le tableau r√©sum√©\n",
    "print(\"\\nListe des colonnes :\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d44c7f",
   "metadata": {},
   "source": [
    "# V√©rification des doublons et valeurs manquantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le nombre de doublons bas√©s sur l'ID\n",
    "nb_duplicated = df.duplicated(subset='ID').sum()\n",
    "print(f\"Nombre de doublons bas√©s sur 'ID' : {nb_duplicated}\")\n",
    "\n",
    "# V√©rifier le pourcentage de valeurs manquantes par colonne\n",
    "missing_ratio = df.isnull().mean().sort_values(ascending=False) * 100\n",
    "print(\"Pourcentage de valeurs manquantes par colonne (top 10) :\")\n",
    "print(missing_ratio.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e5ac8",
   "metadata": {},
   "source": [
    "# Conversion des colonnes temporelles + cr√©ation de Duration(min)\n",
    "## Objectifs :\n",
    "- Convertir Start_Time et End_Time en datetime\n",
    "- Cr√©er la dur√©e en minutes (Duration(min))\n",
    "- Filtrer les dur√©es aberrantes (exemple : n√©gatives ou trop extr√™mes si n√©cessaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41728d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir et calculer pour inspecter\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "df['End_Time'] = pd.to_datetime(df['End_Time'], errors='coerce')\n",
    "\n",
    "# Cr√©ation de la dur√©e en minutes\n",
    "df['Duration(min)'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60\n",
    "\n",
    "# V√©rifier les stats de dur√©e pour d√©cider ensuite quoi faire\n",
    "print(df['Duration(min)'].describe())\n",
    "\n",
    "# V√©rifier combien de lignes ont une dur√©e n√©gative ou extr√™me (> 2 jours)\n",
    "print(\"\\nNombre de lignes avec dur√©e n√©gative :\", (df['Duration(min)'] < 0).sum())\n",
    "print(\"Nombre de lignes avec dur√©e > 2 jours :\", (df['Duration(min)'] > 2880).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b05c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier combien de lignes ont Start_Time ou End_Time manquant\n",
    "missing_start = df['Start_Time'].isnull().sum()\n",
    "missing_end = df['End_Time'].isnull().sum()\n",
    "\n",
    "print(f\"Lignes sans Start_Time : {missing_start}\")\n",
    "print(f\"Lignes sans End_Time : {missing_end}\")\n",
    "\n",
    "# Supprimer ces lignes car sans datetime = pas de dur√©e => inutile pour tableau de bord\n",
    "initial_shape = df.shape\n",
    "df = df.dropna(subset=['Start_Time', 'End_Time'])\n",
    "print(f\"Lignes supprim√©es car Start/End manquant : {initial_shape[0] - df.shape[0]}\")\n",
    "\n",
    "# Filtrer les dur√©es extr√™mes (> 2 jours) si n√©cessaire\n",
    "print(f\"Lignes avant filtrage des dur√©es extr√™mes : {df.shape[0]}\")\n",
    "df = df[df['Duration(min)'] <= 2880]\n",
    "print(f\"Lignes apr√®s filtrage des dur√©es extr√™mes : {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d3739",
   "metadata": {},
   "source": [
    "# Distribution de la gravit√© (Severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=df, x='Severity', order=sorted(df['Severity'].unique()))\n",
    "plt.title('R√©partition des niveaux de gravit√© (Severity)')\n",
    "plt.xlabel('Gravit√©')\n",
    "plt.ylabel('Nombre d\\'accidents')\n",
    "plt.show()\n",
    "\n",
    "# R√©partition en pourcentage pour contr√¥le rapide\n",
    "severity_counts = df['Severity'].value_counts(normalize=True) * 100\n",
    "print(\"Proportion de chaque gravit√© (%):\")\n",
    "print(severity_counts.sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5353615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la dur√©e des accidents (log-scale pour lisibilit√©)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['Duration(min)'], bins=100)\n",
    "plt.yscale('log')\n",
    "plt.title(\"Distribution des dur√©es d'accidents (minutes)\")\n",
    "plt.xlabel(\"Dur√©e (min)\")\n",
    "plt.ylabel(\"Nombre d'incidents (log scale)\")\n",
    "plt.show()\n",
    "\n",
    "# Afficher les principaux percentiles pour une synth√®se rapide\n",
    "duration_percentiles = df['Duration(min)'].quantile([0.5, 0.75, 0.9, 0.95, 0.99])\n",
    "print(\"Percentiles cl√©s des dur√©es (min):\")\n",
    "print(duration_percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c601e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er la colonne HourOfDay\n",
    "df['HourOfDay'] = df['Start_Time'].dt.hour\n",
    "\n",
    "# Afficher la r√©partition num√©rique\n",
    "hour_counts = df['HourOfDay'].value_counts().sort_index()\n",
    "print(\"R√©partition du nombre d'accidents par heure :\")\n",
    "print(hour_counts)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(data=df, x='HourOfDay', color='steelblue')\n",
    "plt.title(\"Nombre d'accidents par heure de la journ√©e\")\n",
    "plt.xlabel(\"Heure (0-23)\")\n",
    "plt.ylabel(\"Nombre d'incidents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d693ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er la colonne DayOfWeek\n",
    "df['DayOfWeek'] = df['Start_Time'].dt.dayofweek  # 0 = lundi, 6 = dimanche\n",
    "\n",
    "# Afficher la r√©partition num√©rique\n",
    "dow_counts = df['DayOfWeek'].value_counts().sort_index()\n",
    "print(\"R√©partition du nombre d'accidents par jour de la semaine (0=Lundi) :\")\n",
    "print(dow_counts)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(data=df, x='DayOfWeek', color='coral')\n",
    "plt.title(\"Nombre d'accidents par jour de la semaine\")\n",
    "plt.xlabel(\"Jour de la semaine (0 = Lundi)\")\n",
    "plt.ylabel(\"Nombre d'incidents\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter Month et Season \n",
    "\n",
    "# Cr√©er la colonne Month\n",
    "df['Month'] = df['Start_Time'].dt.month\n",
    "\n",
    "# Cr√©er la colonne Season selon des r√®gles simples (h√©misph√®re nord)\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "df['Season'] = df['Month'].apply(get_season)\n",
    "\n",
    "# V√©rifier les r√©partitions num√©riques \n",
    "\n",
    "print(\"\\nR√©partition du nombre d'accidents par mois :\")\n",
    "print(df['Month'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nR√©partition du nombre d'accidents par saison :\")\n",
    "print(df['Season'].value_counts())\n",
    "\n",
    "# Visualiser \n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.countplot(data=df, x='Month', color='skyblue')\n",
    "plt.title(\"Nombre d'accidents par mois\")\n",
    "plt.xlabel(\"Mois\")\n",
    "plt.ylabel(\"Nombre d'incidents\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(data=df, x='Season', order=['Winter', 'Spring', 'Summer', 'Fall'], palette='Set2')\n",
    "plt.title(\"Nombre d'accidents par saison\")\n",
    "plt.xlabel(\"Saison\")\n",
    "plt.ylabel(\"Nombre d'incidents\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lectionner les colonnes num√©riques pertinentes\n",
    "num_cols = [\n",
    "    'Severity', 'Duration(min)', \n",
    "    'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', \n",
    "    'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', \n",
    "    'Precipitation(in)'\n",
    "]\n",
    "\n",
    "# V√©rifier les colonnes pr√©sentes\n",
    "print(f\"Colonnes analys√©es : {num_cols}\")\n",
    "\n",
    "# Calculer la matrice de corr√©lation (Spearman plus robuste pour donn√©es non lin√©aires)\n",
    "corr_matrix = df[num_cols].corr(method='spearman')\n",
    "\n",
    "# Afficher la heatmap\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Corr√©lation Spearman entre facteurs et gravit√© (Severity)\")\n",
    "plt.show()\n",
    "\n",
    "# Extraire et afficher le top 5 des facteurs corr√©l√©s √† la gravit√© (hors self-corr√©lation)\n",
    "severity_corr = corr_matrix['Severity'].drop('Severity').abs().sort_values(ascending=False)\n",
    "print(\"\\nTop 5 des variables les plus corr√©l√©es √† la gravit√© :\")\n",
    "print(severity_corr.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e21a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot du top 5 √† partir de severity_corr\n",
    "top_corr = severity_corr.head(5)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=top_corr.values, y=top_corr.index, palette=\"Blues_d\")\n",
    "plt.title(\"Top 5 des facteurs corr√©l√©s √† la gravit√© (|corr|)\")\n",
    "plt.xlabel(\"Coefficient de corr√©lation absolu (Spearman)\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.show()\n",
    "\n",
    "# Afficher les valeurs exactes pour contr√¥le\n",
    "print(\"Top 5 des facteurs corr√©l√©s √† la gravit√© :\")\n",
    "print(top_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation Severity sur [0,1] (1-4 --> 0-1)\n",
    "df['Severity_norm'] = (df['Severity'] - 1) / 3\n",
    "\n",
    "# Normalisation Duration sur [0,1] avec seuil max √† 2880 min (2 jours)\n",
    "df['Duration_norm'] = df['Duration(min)'] / 2880\n",
    "df['Duration_norm'] = df['Duration_norm'].clip(0, 1)\n",
    "\n",
    "# Marquer une condition m√©t√©o risqu√©e (simple) : pluie, neige, brouillard, orage\n",
    "risky_conditions = ['Rain', 'Snow', 'Thunderstorm', 'Fog', 'Heavy Rain', 'Heavy Snow', 'Blowing Snow']\n",
    "df['Weather_risk'] = df['Weather_Condition'].apply(\n",
    "    lambda x: any([kw.lower() in str(x).lower() for kw in risky_conditions])\n",
    ").astype(int)\n",
    "\n",
    "# Pond√©ration : parts √©gales\n",
    "df['Risk_Score'] = (df['Severity_norm'] + df['Duration_norm'] + df['Weather_risk']) / 3\n",
    "\n",
    "# Contr√¥le : aper√ßu\n",
    "print(df[['Severity', 'Duration(min)', 'Weather_Condition', 'Severity_norm', 'Duration_norm', 'Weather_risk', 'Risk_Score']].head())\n",
    "print(\"\\nStatistiques du Risk_Score :\")\n",
    "print(df['Risk_Score'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b90a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regrouper par mois\n",
    "df['Month'] = df['Start_Time'].dt.month\n",
    "\n",
    "resilience_month = df.groupby('Month')['Risk_Score'].mean().reset_index()\n",
    "resilience_month['Resilience_Index'] = 1 - resilience_month['Risk_Score']\n",
    "\n",
    "print(resilience_month)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.lineplot(data=resilience_month, x='Month', y='Resilience_Index', marker='o')\n",
    "plt.title(\"Indice de R√©silience Logistique par Mois (1 - Risk_Score moyen)\")\n",
    "plt.xlabel(\"Mois\")\n",
    "plt.ylabel(\"Resilience Index\")\n",
    "plt.ylim(0.79, 0.84)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d01532",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10,5))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Mois')\n",
    "ax1.set_ylabel('Risk_Score moyen', color=color)\n",
    "ax1.plot(resilience_month['Month'], resilience_month['Risk_Score'], color=color, marker='o')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Resilience Index', color=color)\n",
    "ax2.plot(resilience_month['Month'], resilience_month['Resilience_Index'], color=color, marker='s')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title(\"Risk_Score & Resilience Index par Mois\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap Risk_Score : Mois vs Jour de la semaine\n",
    "\n",
    "# Recalculer si n√©cessaire\n",
    "df['Month'] = df['Start_Time'].dt.month\n",
    "df['DayOfWeek'] = df['Start_Time'].dt.dayofweek  # 0 = Lundi\n",
    "\n",
    "# Grouper\n",
    "heatmap_data = df.groupby(['Month', 'DayOfWeek'])['Risk_Score'].mean().reset_index()\n",
    "\n",
    "# Pivot pour format matrice\n",
    "heatmap_matrix = heatmap_data.pivot(index='DayOfWeek', columns='Month', values='Risk_Score')\n",
    "\n",
    "print(\"\\nRisk_Score moyen par Mois et Jour de la semaine :\")\n",
    "print(heatmap_matrix.round(4))\n",
    "\n",
    "# Afficher la heatmap\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(heatmap_matrix, annot=True, fmt=\".3f\", cmap=\"YlOrRd\")\n",
    "plt.title(\"Heatmap Risk_Score moyen (Mois vs Jour de la semaine)\")\n",
    "plt.xlabel(\"Mois\")\n",
    "plt.ylabel(\"Jour de la semaine (0 = Lundi)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une colonne Resilience_Index = 1 - Risk_Score\n",
    "df['Resilience_Index'] = 1 - df['Risk_Score']\n",
    "\n",
    "# Grouper pour la heatmap\n",
    "heatmap_resilience = df.groupby(['Month', 'DayOfWeek'])['Resilience_Index'].mean().reset_index()\n",
    "heatmap_resilience_matrix = heatmap_resilience.pivot(index='DayOfWeek', columns='Month', values='Resilience_Index')\n",
    "\n",
    "print(\"\\nResilience Index moyen par Mois et Jour de la semaine :\")\n",
    "print(heatmap_resilience_matrix.round(4))\n",
    "\n",
    "# Afficher\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(heatmap_resilience_matrix, annot=True, fmt=\".3f\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Heatmap Resilience Index moyen (Mois vs Jour de la semaine)\")\n",
    "plt.xlabel(\"Mois\")\n",
    "plt.ylabel(\"Jour de la semaine (0 = Lundi)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7539bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er la colonne 'HourOfDay' √† partir de Start_Time\n",
    "df['HourOfDay'] = df['Start_Time'].dt.hour\n",
    "\n",
    "# Calculer Risk_Score moyen et Resilience_Index par heure\n",
    "risk_hour = df.groupby('HourOfDay')['Risk_Score'].mean().reset_index()\n",
    "risk_hour['Resilience_Index'] = 1 - risk_hour['Risk_Score']\n",
    "\n",
    "print(\"\\nRisk_Score et Resilience_Index par heure :\")\n",
    "print(risk_hour)\n",
    "\n",
    "# Afficher : ligne bleue = Risk_Score, ligne rouge = Resilience_Index\n",
    "fig, ax1 = plt.subplots(figsize=(12,5))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Heure de la journ√©e')\n",
    "ax1.set_ylabel('Risk_Score moyen', color=color)\n",
    "ax1.plot(risk_hour['HourOfDay'], risk_hour['Risk_Score'], marker='o', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_xticks(range(0,24))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Resilience_Index', color=color)\n",
    "ax2.plot(risk_hour['HourOfDay'], risk_hour['Resilience_Index'], marker='s', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title(\"Risk_Score & Resilience_Index par heure de la journ√©e\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir des cat√©gories de risque logistique bas√©es sur la gravit√©, dur√©e, m√©t√©o et heure\n",
    "\n",
    "def assign_risk_category(row):\n",
    "    if (row['Severity'] >= 3) and (row['Duration(min)'] > 120):\n",
    "        return 'High Infrastructure Block'\n",
    "    elif row['Weather_Condition'] in ['Heavy Rain', 'Snow', 'Thunderstorm', 'Fog']:\n",
    "        return 'Weather Disruption'\n",
    "    elif row['HourOfDay'] in range(7,10) or row['HourOfDay'] in range(16,20):\n",
    "        return 'Peak Hour Congestion'\n",
    "    else:\n",
    "        return 'Low Impact'\n",
    "\n",
    "# S'assurer qu'on a bien la colonne HourOfDay\n",
    "df['HourOfDay'] = df['Start_Time'].dt.hour\n",
    "\n",
    "# Appliquer la fonction\n",
    "df['Risk_Category'] = df.apply(assign_risk_category, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476da900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul final propre\n",
    "risk_summary = df['Risk_Category'].value_counts().reset_index()\n",
    "risk_summary.columns = ['Risk_Category', 'Count']\n",
    "risk_summary['Proportion (%)'] = (risk_summary['Count'] / df.shape[0] * 100).round(2)\n",
    "\n",
    "print(\"\\nR√©partition finale des cat√©gories de risque logistique\")\n",
    "print(risk_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e12f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    data=risk_summary, \n",
    "    y=\"Risk_Category\", \n",
    "    x=\"Count\", \n",
    "    hue=\"Risk_Category\",  # Fix pour √©viter le FutureWarning\n",
    "    dodge=False,          # pour √©viter le doublement\n",
    "    palette=\"Set2\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"R√©partition finale des types de risque logistique (cat√©gories)\")\n",
    "plt.xlabel(\"Nombre d'incidents\")\n",
    "plt.ylabel(\"Cat√©gorie de risque\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyenne par mois\n",
    "summary_month = df.groupby('Month')['Risk_Score'].agg(['mean']).reset_index()\n",
    "summary_month.rename(columns={'mean': 'Risk_Score_mean'}, inplace=True)\n",
    "summary_month['Resilience_Index_mean'] = 1 - summary_month['Risk_Score_mean']\n",
    "\n",
    "# Moyenne par jour de la semaine\n",
    "summary_dayofweek = df.groupby('DayOfWeek')['Risk_Score'].agg(['mean']).reset_index()\n",
    "summary_dayofweek.rename(columns={'mean': 'Risk_Score_mean'}, inplace=True)\n",
    "summary_dayofweek['Resilience_Index_mean'] = 1 - summary_dayofweek['Risk_Score_mean']\n",
    "\n",
    "# Moyenne par heure de la journ√©e\n",
    "summary_hourofday = df.groupby('HourOfDay')['Risk_Score'].agg(['mean']).reset_index()\n",
    "summary_hourofday.rename(columns={'mean': 'Risk_Score_mean'}, inplace=True)\n",
    "summary_hourofday['Resilience_Index_mean'] = 1 - summary_hourofday['Risk_Score_mean']\n",
    "\n",
    "# Moyenne par cat√©gorie de risque\n",
    "summary_category = df.groupby('Risk_Category')['Risk_Score'].agg(['mean', 'count']).reset_index()\n",
    "summary_category.rename(columns={'mean': 'Risk_Score_mean', 'count': 'Count'}, inplace=True)\n",
    "summary_category['Resilience_Index_mean'] = 1 - summary_category['Risk_Score_mean']\n",
    "summary_category['Proportion'] = summary_category['Count'] / summary_category['Count'].sum()\n",
    "\n",
    "# Afficher un √©chantillon pour v√©rif rapide\n",
    "print(\"\\nMoyenne par mois ===\")\n",
    "print(summary_month)\n",
    "\n",
    "print(\"\\nMoyenne par jour de la semaine ===\")\n",
    "print(summary_dayofweek)\n",
    "\n",
    "print(\"\\nMoyenne par heure de la journ√©e ===\")\n",
    "print(summary_hourofday)\n",
    "\n",
    "print(\"\\nMoyenne par cat√©gorie de risque ===\")\n",
    "print(summary_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6298b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour graphe par mois\n",
    "risk_by_month = df.groupby('Month')['Risk_Score'].mean()\n",
    "resilience_by_month = 1 - risk_by_month\n",
    "\n",
    "# Idem pour jour de semaine et heure\n",
    "risk_by_day = df.groupby('DayOfWeek')['Risk_Score'].mean()\n",
    "resilience_by_day = 1 - risk_by_day\n",
    "\n",
    "risk_by_hour = df.groupby('HourOfDay')['Risk_Score'].mean()\n",
    "resilience_by_hour = 1 - risk_by_hour\n",
    "\n",
    "risk_by_cat = df.groupby('Risk_Category')['Risk_Score'].mean()\n",
    "resilience_by_cat = 1 - risk_by_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a4a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "# D√©finir palette\n",
    "risk_color = \"#e74c3c\"\n",
    "resilience_color = \"#27ae60\"\n",
    "\n",
    "# Risque & R√©silience par Mois ---\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df_month['Month'], df_month['Risk_Score_mean'], '-o', label='Risque', color=risk_color, linewidth=2.5)\n",
    "plt.plot(df_month['Month'], df_month['Resilience_Index_mean'], '-o', label='R√©silience', color=resilience_color, linewidth=2.5)\n",
    "\n",
    "plt.title(\"Indice de Risque et de R√©silience par Mois\")\n",
    "plt.xlabel(\"Mois\")\n",
    "plt.ylabel(\"Score (0-1)\")\n",
    "plt.xticks(df_month['Month'])\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Risque & R√©silience par Jour de la semaine ---\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df_day['DayOfWeek'], df_day['Risk_Score_mean'], '-o', label='Risque', color=risk_color, linewidth=2.5)\n",
    "plt.plot(df_day['DayOfWeek'], df_day['Resilience_Index_mean'], '-o', label='R√©silience', color=resilience_color, linewidth=2.5)\n",
    "\n",
    "plt.title(\"Indice de Risque et de R√©silience par Jour de la Semaine (0 = Lundi)\")\n",
    "plt.xlabel(\"Jour de la Semaine\")\n",
    "plt.ylabel(\"Score (0-1)\")\n",
    "plt.xticks(df_day['DayOfWeek'], labels=[\"Lun\", \"Mar\", \"Mer\", \"Jeu\", \"Ven\", \"Sam\", \"Dim\"])\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Risque & R√©silience par Heure ---\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df_hour['HourOfDay'], df_hour['Risk_Score_mean'], '-o', label='Risque', color=risk_color, linewidth=2.5)\n",
    "plt.plot(df_hour['HourOfDay'], df_hour['Resilience_Index_mean'], '-o', label='R√©silience', color=resilience_color, linewidth=2.5)\n",
    "\n",
    "plt.title(\"Indice de Risque et de R√©silience par Heure de la Journ√©e\")\n",
    "plt.xlabel(\"Heure\")\n",
    "plt.ylabel(\"Score (0-1)\")\n",
    "plt.xticks(df_hour['HourOfDay'])\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Risque & R√©silience par Cat√©gorie ---\n",
    "plt.figure(figsize=(12,6))\n",
    "bar_width = 0.35\n",
    "x = np.arange(len(df_cat['Risk_Category']))\n",
    "\n",
    "plt.bar(x - bar_width/2, df_cat['Risk_Score_mean'], width=bar_width, color=risk_color, label='Risque')\n",
    "plt.bar(x + bar_width/2, df_cat['Resilience_Index_mean'], width=bar_width, color=resilience_color, label='R√©silience')\n",
    "\n",
    "plt.title(\"Comparaison Risque & R√©silience par Cat√©gorie de Risque Logistique\")\n",
    "plt.xlabel(\"Cat√©gorie\")\n",
    "plt.ylabel(\"Score (0-1)\")\n",
    "plt.xticks(x, df_cat['Risk_Category'], rotation=20)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80453368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer Weather Disruption et High Infrastructure Block\n",
    "weather_df = df[df['Risk_Category'] == 'Weather Disruption'].copy()\n",
    "infra_block_df = df[df['Risk_Category'] == 'High Infrastructure Block'].copy()\n",
    "\n",
    "# Afficher nombre de lignes\n",
    "print(f\"Weather Disruption : {weather_df.shape[0]} incidents\")\n",
    "print(f\"High Infrastructure Block : {infra_block_df.shape[0]} incidents\")\n",
    "\n",
    "# Afficher un √©chantillon\n",
    "print(\"\\nWeather Disruption sample :\")\n",
    "print(weather_df.head(3))\n",
    "\n",
    "print(\"\\nHigh Infrastructure Block sample :\")\n",
    "print(infra_block_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par mois\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "sns.countplot(data=weather_df, x='Month', ax=ax[0])\n",
    "ax[0].set_title(\"Weather Disruption - par Mois\")\n",
    "sns.countplot(data=infra_block_df, x='Month', ax=ax[1])\n",
    "ax[1].set_title(\"High Infrastructure Block - par Mois\")\n",
    "plt.show()\n",
    "\n",
    "# Par jour de la semaine\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "sns.countplot(data=weather_df, x='DayOfWeek', ax=ax[0])\n",
    "ax[0].set_title(\"Weather Disruption - par Jour\")\n",
    "sns.countplot(data=infra_block_df, x='DayOfWeek', ax=ax[1])\n",
    "ax[1].set_title(\"High Infrastructure Block - par Jour\")\n",
    "plt.show()\n",
    "\n",
    "# Par heure\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "sns.countplot(data=weather_df, x='HourOfDay', ax=ax[0])\n",
    "ax[0].set_title(\"Weather Disruption - par Heure\")\n",
    "sns.countplot(data=infra_block_df, x='HourOfDay', ax=ax[1])\n",
    "ax[1].set_title(\"High Infrastructure Block - par Heure\")\n",
    "plt.show()\n",
    "\n",
    "# === B) Conditions m√©t√©o ===\n",
    "print(\"\\nTop 10 Weather Conditions pour Weather Disruption :\")\n",
    "print(weather_df['Weather_Condition'].value_counts().head(10))\n",
    "\n",
    "# === C) Statistiques Dur√©e et Gravit√© ===\n",
    "print(\"\\nStats Weather Disruption:\")\n",
    "print(weather_df[['Duration(min)', 'Severity']].describe())\n",
    "\n",
    "print(\"\\nStats High Infrastructure Block:\")\n",
    "print(infra_block_df[['Duration(min)', 'Severity']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb7bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserver uniquement incidents avec Weather Disruption OU Peak Hour Congestion\n",
    "df_heat = df[df['Risk_Category'].isin(['Peak Hour Congestion', 'Weather Disruption'])].copy()\n",
    "\n",
    "# Remplacer 'None' par 'Clear/Other' pour contr√¥le mais on peut filtrer ensuite\n",
    "df_heat = df_heat[df_heat['Main_Weather'] != 'None']\n",
    "\n",
    "# Si tu veux limiter aux types m√©t√©o cl√©s uniquement (Rain, Snow, Fog, Thunderstorm)\n",
    "df_heat = df_heat[df_heat['Main_Weather'].isin(['Rain', 'Snow', 'Fog', 'Thunderstorm'])]\n",
    "\n",
    "# Regrouper Risk_Score moyen par heure et m√©t√©o\n",
    "heatmap_data = df_heat.groupby(['Main_Weather', 'HourOfDay'])['Risk_Score'].mean().reset_index()\n",
    "\n",
    "# Pivot pour heatmap\n",
    "heatmap_matrix = heatmap_data.pivot(index='Main_Weather', columns='HourOfDay', values='Risk_Score')\n",
    "\n",
    "# Tri de l'index pour l'ordre que tu veux\n",
    "heatmap_matrix = heatmap_matrix.loc[['Fog', 'Rain', 'Snow', 'Thunderstorm']]\n",
    "\n",
    "# Afficher\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.heatmap(heatmap_matrix, annot=True, fmt=\".2f\", cmap=\"YlOrRd\")\n",
    "plt.title(\"Heatmap Risque moyen (Weather Conditions vs Heure de la journ√©e)\")\n",
    "plt.xlabel(\"Heure\")\n",
    "plt.ylabel(\"Condition m√©t√©o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtiquette m√©t√©o simplifi√©e pour ceux affect√©s par la m√©t√©o\n",
    "conditions = [\n",
    "    df['Risk_Category'] == 'Weather Disruption',\n",
    "]\n",
    "choices = [\n",
    "    df['Weather_Condition'].where(df['Risk_Category'] == 'Weather Disruption', None)\n",
    "]\n",
    "\n",
    "df['Main_Weather'] = np.select(conditions, choices, default=None)\n",
    "\n",
    "# Nettoyer : regrouper les valeurs proches\n",
    "df['Main_Weather'] = df['Main_Weather'].replace({\n",
    "    'Heavy Rain': 'Rain',\n",
    "    'Light Rain': 'Rain',\n",
    "    'Snow': 'Snow',\n",
    "    'Heavy Snow': 'Snow',\n",
    "    'Thunderstorm': 'Thunderstorm',\n",
    "    'Fog': 'Fog'\n",
    "}).fillna('None')\n",
    "\n",
    "# V√©rif : risk & resilience sont d√©j√† calcul√©s ===\n",
    "df['Resilience_Index'] = 1 - df['Risk_Score']\n",
    "\n",
    "# V√©rif : cat√©gorie risque d√©j√† correcte ===\n",
    "print(df['Risk_Category'].value_counts())\n",
    "\n",
    "\n",
    "# Export \n",
    "cols_to_export = [\n",
    "    'ID', 'Start_Time', 'Severity', 'Duration(min)',\n",
    "    'Risk_Score', 'Resilience_Index', 'Risk_Category',\n",
    "    'Main_Weather',\n",
    "    'Month', 'DayOfWeek', 'HourOfDay'\n",
    "]\n",
    "\n",
    "output_path = \"../data/cleaned/usa_accidents_traffic_cleaned.csv\"\n",
    "df[cols_to_export].to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Export fait : {output_path}\")\n",
    "print(f\"Colonnes export√©es : {cols_to_export}\")\n",
    "print(f\"Lignes : {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a6365",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df.shape[0]\n",
    "peak_hour_count = df[df['Risk_Category'] == 'Peak Hour Congestion'].shape[0]\n",
    "infra_block_mean = infra_block_df['Duration(min)'].mean()\n",
    "weather_count = weather_df.shape[0]\n",
    "\n",
    "peak_hour_pct = peak_hour_count / total * 100\n",
    "weather_pct = weather_count / total * 100\n",
    "\n",
    "print(f\"\"\"\n",
    "Synth√®se du risque routier :\n",
    "- Plus de {peak_hour_pct:.1f}% des incidents surviennent lors des heures de pointe, ce qui en fait la principale cause de congestion logistique.\n",
    "- Les incidents ¬´ Infrastructure Bloqu√©e ¬ª repr√©sentent des blocages longs, avec une dur√©e moyenne de {infra_block_mean:.0f} minutes.\n",
    "- Les conditions m√©t√©o s√©v√®res sont moins fr√©quentes ({weather_pct:.1f}% des cas) mais g√©n√®rent un risque plus √©lev√©, notamment en cas de neige, brouillard ou orage.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5169a",
   "metadata": {},
   "source": [
    "# R√©sum√© final du module Analyse du Risque Routier\n",
    "\n",
    "## Objectif\n",
    "Ce notebook a permis de nettoyer, structurer et analyser le dataset US Accidents (2016‚Äì2023)\n",
    "dans le cadre du pipeline R&D pour mesurer la r√©silience logistique.\n",
    "L'objectif principal est de fournir un fichier pr√™t √† √™tre int√©gr√© dans un tableau de bord.\n",
    "\n",
    "## Ce qui a √©t√© fait\n",
    "- Chargement et nettoyage des donn√©es : conversion des timestamps, calcul de la dur√©e en minutes,\n",
    "  suppression des valeurs aberrantes (dur√©es n√©gatives ou sup√©rieures √† 2 jours).\n",
    "- Exploration descriptive : distribution des niveaux de gravit√©, distribution des dur√©es,\n",
    "  identification des pics horaires, jours et mois √† risque.\n",
    "- Corr√©lation Spearman pour d√©tecter les variables influentes sur la gravit√©.\n",
    "- Cr√©ation d'un score de risque standardis√© (Risk_Score) combinant gravit√©, dur√©e et m√©t√©o.\n",
    "- Calcul d'un indice de r√©silience (Resilience_Index) comme compl√©ment de l'indice de risque.\n",
    "- Attribution d'une cat√©gorie de risque logistique pour chaque incident :\n",
    "  - Peak Hour Congestion (heures de pointe)\n",
    "  - High Infrastructure Block (incidents bloquants longue dur√©e)\n",
    "  - Weather Disruption (conditions m√©t√©o critiques)\n",
    "  - Low Impact (autres cas mineurs)\n",
    "- G√©n√©ration de visualisations pour valider la saisonnalit√©, la cyclicit√© horaire et les pics par cat√©gorie.\n",
    "- Export final d'un fichier CSV structur√© contenant toutes les colonnes utiles :\n",
    "  ID, Start_Time, Severity, Duration(min), Risk_Score, Resilience_Index, Risk_Category,\n",
    "  Main_Weather (regroupement des conditions m√©t√©o dominantes), Month, DayOfWeek, HourOfDay.\n",
    "\n",
    "## Analyse synth√©tique\n",
    "- La majorit√© des incidents sont class√©s en Low Impact, avec un risque faible pour la logistique.\n",
    "- Les congestions horaires (matin et fin de journ√©e) repr√©sentent environ 42% des incidents.\n",
    "- Les incidents bloquants (High Infrastructure Block) sont rares (environ 3%) mais leur dur√©e\n",
    "  moyenne est √©lev√©e (~300 min), ce qui impacte fortement la fluidit√© des r√©seaux.\n",
    "- Les perturbations li√©es √† la m√©t√©o (Weather Disruption) sont limit√©es (environ 2% des cas) mais\n",
    "  concentr√©es sur des √©v√©nements de forte pluie, neige ou brouillard.\n",
    "\n",
    "## Conclusion\n",
    "Ce module fournit une base coh√©rente pour alimenter un tableau de bord op√©rationnel permettant\n",
    "de suivre en temps r√©el ou en historique les risques routiers pour la cha√Æne logistique.\n",
    "Les indicateurs produits sont pr√™ts pour une visualisation interactive et un croisement √©ventuel\n",
    "avec d'autres modes de transport."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
