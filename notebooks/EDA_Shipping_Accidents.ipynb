{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d3c007",
   "metadata": {},
   "source": [
    "# Accidents cargo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9382550c",
   "metadata": {},
   "source": [
    "## Import des dépendances et du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Chemin vers le .shp\n",
    "shp_path = \"../data/extracted/Shipping_Accidents/Shipping_Accidents.shp\"\n",
    "\n",
    "# Lecture du shapefile avec geopandas\n",
    "df = gpd.read_file(shp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11b3e1",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d545183a",
   "metadata": {},
   "source": [
    "### Nettoyage des données\n",
    "\n",
    "1. Les données allant des années 1989 à 2023, nous allons les filtrer et ne garder qu'une intervalle de 20 ans, soit de 2003 à 2023. Cela nous permettra de ne pas avoir des données trop anciennes qui pourraient fausser notre analyse.\n",
    "\n",
    "2. Le nombre de types d'accidents étant important voir redondant, nous allons les regrouper en 5 catégories :\n",
    "   - Technical or Equipment Failure\n",
    "   - Navigation or Maneuvering Incident\n",
    "   - Fire or Explosion\n",
    "   - Life-saving Equipment Incident\n",
    "   - Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Définition de l'intervalle de temps\n",
    "start_year = 2003\n",
    "end_year = 2023\n",
    "\n",
    "df = df[(df['Year'] >= start_year) & (df['Year'] <= end_year)]\n",
    "\n",
    "\n",
    "## Suppression des coordonnées incohérentes\n",
    "# Suppression des lignes où les coordonnées sont nulles ou égales à 0\n",
    "df = df[~((df['Longitude'].isnull()) | (df['Latitude'].isnull()) | (df['Longitude'] == 0) | (df['Latitude'] == 0))]\n",
    "\n",
    "\n",
    "## Regroupement des types d'accidents\n",
    "# Défaillance technique ou équipement\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Damage to ship or equipment', 'Défaillance technique ou équipement')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Damages to ships or equipment', 'Défaillance technique ou équipement')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Dammage to ships or equipment', 'Défaillance technique ou équipement')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Door fault . fault in doorways', 'Défaillance technique ou équipement')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('hull failure', 'Défaillance technique ou équipement')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Hull failure/failure of watertight doors/ports etc.', 'Défaillance technique ou équipement')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('machinery damage', 'Défaillance technique ou équipement')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Machinery damage', 'Défaillance technique ou équipement')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Machinery dammage', 'Défaillance technique ou équipement')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('macihnery damage', 'Défaillance technique ou équipement')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Technical failure', 'Défaillance technique ou équipement')\n",
    "\n",
    "# Navigation or Maneuvering Incident\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Capsizing.listing', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Capsizing/listing', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('collision', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Collision', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('contact', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Contact', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Flooding/Foundering', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('grounding', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Grounding', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Grounding/stranding', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Loss of control', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('stranding.grounding', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Stranding.grounding', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('stranding/grounding', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Stranding/grounding', 'Erreur de navigation ou de manoeuvre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Tilt / crash', 'Erreur de navigation ou de manoeuvre')\n",
    "\n",
    "# Fire or Explosion\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Fire', 'Incendie ou explosion')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Fire . explosion', 'Incendie ou explosion')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Fire/Explosion', 'Incendie ou explosion')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Fire or explosion', 'Incendie ou explosion')\n",
    "\n",
    "\n",
    "# Life-saving Equipment Incident\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Accidents with life-saving appliances', 'Équipement de sauvetage')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Related to the use of rescue equipment', 'Équipement de sauvetage')\n",
    "\n",
    "# Other\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('n.i.', 'Autre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('other', 'Autre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Other', 'Autre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace(\"other (unsealing the vessel's hull)\", 'Autre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Other reason', 'Autre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Other type', 'Autre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Physical damage', 'Autre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('Sunk', 'Autre')\n",
    "df['Acc_Type'] = df['Acc_Type'].replace('v.serious accident', 'Autre')\n",
    "df['Acc_Type'] = df['Acc_Type'].fillna('Autre')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba22486e",
   "metadata": {},
   "source": [
    "### Enrichissement des données\n",
    "\n",
    "La colonne `Location` n'étant pas toujours renseignée, nous allons la compléter en utilisant deux autres datassets :\n",
    "- World Port Index – Port Data\n",
    "- Natural Earth\n",
    "Ces deux datasets contiennent des informations sur les ports et les côtes du monde entier, ce qui nous permettra par croisement de données de compléter les informations manquantes dans notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d719be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion du DataFrame en GeoDataFrame avec CRS standard\n",
    "df.to_crs(\"EPSG:4326\", inplace=True)\n",
    "gdf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "# Chargement des shapefiles\n",
    "lands = gpd.read_file(\"../data/extracted/Shipping_Accidents/lands.shp\")\n",
    "ports = gpd.read_file(\"../data/extracted/Shipping_Accidents/ports.shp\")\n",
    "\n",
    "# Forcer le CRS si non défini\n",
    "for layer in [lands, ports]:\n",
    "    if layer.crs is None:\n",
    "        layer.set_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "# Reprojection en projection métrique (pour calculs de distances)\n",
    "gdf_proj = gdf.to_crs(\"EPSG:3857\")\n",
    "ports_proj = ports.to_crs(\"EPSG:3857\")\n",
    "land_proj = lands.to_crs(\"EPSG:3857\")\n",
    "\n",
    "# Création des buffers\n",
    "buffer_distances = {\n",
    "    \"port\": 3000,        # 3 km\n",
    "    \"approach\": 10000,   # 10 km\n",
    "    \"coast\": 20000       # 20 km\n",
    "}\n",
    "\n",
    "ports_buffer = gpd.GeoDataFrame(geometry=ports_proj.buffer(buffer_distances[\"port\"]), crs=ports_proj.crs)\n",
    "ports_approach_buffer = gpd.GeoDataFrame(geometry=ports_proj.buffer(buffer_distances[\"approach\"]), crs=ports_proj.crs)\n",
    "coast_buffer = gpd.GeoDataFrame(geometry=land_proj.buffer(buffer_distances[\"coast\"]), crs=land_proj.crs)\n",
    "\n",
    "\n",
    "## Attribution des zones géographiques par jointures spatiales\n",
    "\n",
    "# a) Zone portuaire\n",
    "join_port = gdf_proj.sjoin(ports_buffer, how=\"left\", predicate=\"intersects\")\n",
    "is_port = pd.Series(False, index=gdf_proj.index)  # Série de False par défaut\n",
    "is_port.loc[join_port.index] = join_port[\"index_right\"].notnull()\n",
    "gdf_proj[\"is_port\"] = is_port\n",
    "\n",
    "# b) Zone d’approche portuaire (hors port)\n",
    "join_approach = gdf_proj[~gdf_proj[\"is_port\"]].sjoin(ports_approach_buffer, how=\"left\", predicate=\"intersects\")\n",
    "is_approach = pd.Series(False, index=gdf_proj.index)\n",
    "is_approach.loc[join_approach.index] = join_approach[\"index_right\"].notnull()\n",
    "gdf_proj[\"is_port_approach\"] = is_approach\n",
    "\n",
    "# c) Zone côtière (hors port et approche)\n",
    "mask = (~gdf_proj[\"is_port\"]) & (~gdf_proj[\"is_port_approach\"].fillna(False))\n",
    "join_coast = gdf_proj[mask].sjoin(coast_buffer, how=\"left\", predicate=\"intersects\")\n",
    "is_coastal = pd.Series(False, index=gdf_proj.index)\n",
    "is_coastal.loc[join_coast.index] = join_coast[\"index_right\"].notnull()\n",
    "gdf_proj[\"is_coastal\"] = is_coastal\n",
    "\n",
    "# d) Classification finale de l'emplacement\n",
    "def classify_location(row):\n",
    "    if row[\"is_port\"]:\n",
    "        return \"Port\"\n",
    "    elif row[\"is_port_approach\"]:\n",
    "        return \"En approche du port\"\n",
    "    elif row[\"is_coastal\"]:\n",
    "        return \"Mer\"\n",
    "    else:\n",
    "        return \"Haute mer\"\n",
    "\n",
    "gdf_proj[\"Location\"] = gdf_proj.apply(classify_location, axis=1)\n",
    "\n",
    "\n",
    "## Reprojection finale en WGS84 + mise à jour du DataFrame d’origine\n",
    "\n",
    "df[\"Location\"] = gdf_proj.to_crs(\"EPSG:4326\")[\"Location\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seuil strict de suppression\n",
    "threshold = 0.85\n",
    "\n",
    "# Identification et suppression\n",
    "cols_to_drop = df.columns[df.isnull().mean() > threshold]\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(f\"Colonnes supprimées (> {int(threshold*100)}% de valeurs manquantes) :\")\n",
    "print(list(cols_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cb3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage + remplissage intelligent des colonnes critiques\n",
    "\n",
    "df[\"Damage\"] = (\n",
    "    df[\"Damage\"]\n",
    "    .str.strip().str.title()\n",
    "    .replace({\"Not Known\": None, \"Unknown\": None, \"\": None})\n",
    "    .fillna(\"Non renseigné\")\n",
    ")\n",
    "\n",
    "df[\"Pollu_t\"] = (\n",
    "    df[\"Pollu_t\"]\n",
    "    .str.strip().str.lower()\n",
    "    .replace({\"not known\": None, \"unknown\": None, \"\": None})\n",
    "    .fillna(\"non précisé\")\n",
    ")\n",
    "\n",
    "df[\"Cause_Sh1\"] = (\n",
    "    df[\"Cause_Sh1\"]\n",
    "    .str.strip().str.capitalize()\n",
    "    .replace({\"\": None})\n",
    "    .fillna(\"Non précisé\")\n",
    ")\n",
    "\n",
    "df[\"Pilot_Sh1\"] = (\n",
    "    df[\"Pilot_Sh1\"]\n",
    "    .str.strip().str.lower()\n",
    "    .replace({\"not known\": None, \"unknown\": None, \"\": None})\n",
    "    .fillna(\"inconnu\")\n",
    ")\n",
    "\n",
    "df[\"Assistance\"] = (\n",
    "    df[\"Assistance\"]\n",
    "    .str.strip().str.capitalize()\n",
    "    .fillna(\"Non renseigné\")\n",
    ")\n",
    "\n",
    "df[\"Ship1_Name\"] = (\n",
    "    df[\"Ship1_Name\"]\n",
    "    .str.strip().str.upper()\n",
    "    .fillna(\"NON RENSEIGNE\")\n",
    ")\n",
    "\n",
    "df[\"Colli_Type\"] = df[\"Colli_Type\"].str.strip().str.title().fillna(\"Non précisé\")\n",
    "df[\"Cargo_Type\"] = df[\"Cargo_Type\"].str.strip().str.title().fillna(\"Non précisé\")\n",
    "df[\"IceCondit\"] = df[\"IceCondit\"].str.strip().str.title().fillna(\"Non précisé\")\n",
    "df[\"Sh1_Type\"] = df[\"Sh1_Type\"].str.strip().str.title().fillna(\"Non précisé\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression stricte des colonnes avec trop de valeurs manquantes restantes\n",
    "cols_to_remove_final = [\n",
    "    \"CauseDetai\", \"CrewIceTra\", \"HumanEleme\", \"Sh2Size_gt\",\n",
    "    \"Sh2_Categ\", \"Acc_Detail\", \"Sh1_Hull\", \"Date\"\n",
    "]\n",
    "df.drop(columns=cols_to_remove_final, inplace=True)\n",
    "\n",
    "print(\"Colonnes définitivement supprimées car trop incomplètes :\")\n",
    "print(cols_to_remove_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des colonnes\n",
    "df[\"Decade\"] = (df[\"Year\"] // 10) * 10\n",
    "\n",
    "df[\"Damage_Severe\"] = df[\"Damage\"].isin([\"Severe Damage\", \"Total Loss\"]).astype(int)\n",
    "\n",
    "location_map = {\n",
    "    \"Port\": \"P\",\n",
    "    \"En approche du port\": \"A\",\n",
    "    \"Mer\": \"S\",\n",
    "    \"Haute mer\": \"O\"\n",
    "}\n",
    "df[\"Location_Code\"] = df[\"Location\"].map(location_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d55ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pollution\"] = df[\"Pollution\"].str.strip().str.lower()\n",
    "df[\"Pollution\"] = df[\"Pollution\"].replace({\n",
    "    \"yes\": \"Oui\",\n",
    "    \"no\": \"Non\",\n",
    "    \"n.i.\": \"Non précisé\"\n",
    "})\n",
    "\n",
    "# Sauvegarde brute si besoin\n",
    "df[\"Sh1Draug_raw\"] = df[\"Sh1Draug_m\"]\n",
    "\n",
    "# Nettoyage contrôlé sans perdre les autres données\n",
    "df[\"Sh1Draug_m\"] = df[\"Sh1Draug_m\"].replace(\"n.i.\", \"Non précisé\")\n",
    "\n",
    "# Conversion avec fallback texte\n",
    "df[\"Sh1Draug_m\"] = df[\"Sh1Draug_m\"].apply(lambda x: float(x) if str(x).replace('.', '', 1).isdigit() else \"Non précisé\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abfd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage texte\n",
    "df[\"Damage_clean\"] = df[\"Damage\"].str.strip().str.lower()\n",
    "\n",
    "# Regroupement par classes\n",
    "def classify_damage(text):\n",
    "    if pd.isnull(text) or text in [\"non renseigné\", \"no damage\", \"0\", \"n.i.\"]:\n",
    "        return \"Aucun\"\n",
    "    elif any(kw in text for kw in [\"minor\", \"light\", \"scratches\", \"superficial\"]):\n",
    "        return \"Mineur\"\n",
    "    elif any(kw in text for kw in [\"damage\", \"fracture\", \"hull\", \"propeller\", \"fire\"]):\n",
    "        return \"Modéré\"\n",
    "    elif any(kw in text for kw in [\"severe\", \"flood\", \"sinking\", \"explosion\", \"total\", \"major\"]):\n",
    "        return \"Sévère\"\n",
    "    else:\n",
    "        return \"Inconnu\"\n",
    "\n",
    "df[\"Damage_Class\"] = df[\"Damage_clean\"].apply(classify_damage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c94444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des valeurs textuelles\n",
    "df[\"Pollution\"] = df[\"Pollution\"].str.strip().str.lower()\n",
    "df[\"Pollution\"] = df[\"Pollution\"].replace({\n",
    "    \"yes\": \"Oui\",\n",
    "    \"oui\": \"Oui\",\n",
    "    \"no\": \"Non\",\n",
    "    \"non\": \"Non\",\n",
    "    \"n.i.\": \"Non précisé\",\n",
    "    \"no information\": \"Non précisé\",\n",
    "    \"unknown\": \"Non précisé\"\n",
    "})\n",
    "\n",
    "# Création colonne binaire : Pollution présente ou non\n",
    "df[\"Pollution_Binaire\"] = df[\"Pollution\"].apply(lambda x: 1 if x == \"Oui\" else 0 if x == \"Non\" else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des champs pollution volume\n",
    "df[\"Pollu_t_clean\"] = pd.to_numeric(\n",
    "    df[\"Pollu_t\"].astype(str).str.replace(\",\", \".\").str.extract(r\"([\\d.]+)\")[0],\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "df[\"Pollu_m3_clean\"] = pd.to_numeric(\n",
    "    df[\"Pollu_m3\"].astype(str).str.replace(\",\", \".\").str.extract(r\"([\\d.]+)\")[0],\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Score simple : somme des 2 (à adapter selon besoin)\n",
    "df[\"Pollution_Score\"] = df[\"Pollu_t_clean\"].fillna(0) + df[\"Pollu_m3_clean\"].fillna(0)\n",
    "\n",
    "# Ajout d'un indicateur de complétion du Pollution_Score\n",
    "df[\"Pollution_Score_Complet\"] = df[[\"Pollu_t_clean\", \"Pollu_m3_clean\"]].notnull().all(axis=1).astype(int)\n",
    "\n",
    "# Score pondéré en fonction de la complétion\n",
    "df[\"Pollution_Score_Weighted\"] = df[\"Pollution_Score\"] * df[\"Pollution_Score_Complet\"]\n",
    "\n",
    "# Catégorie de fiabilité du score\n",
    "df[\"Pollution_Qualité\"] = df[\"Pollution_Score_Complet\"].map({1: \"Fiable\", 0: \"Partiel\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36fc8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage du champ taille du navire\n",
    "df[\"Sh1Size_gt_clean\"] = pd.to_numeric(\n",
    "    df[\"Sh1Size_gt\"].astype(str).str.replace(\",\", \".\").str.extract(r\"([\\d.]+)\")[0],\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Création du score brut (score direct égal à la taille)\n",
    "df[\"Ship_Profile_Score\"] = df[\"Sh1Size_gt_clean\"].fillna(0)\n",
    "\n",
    "# Catégorisation des tailles de navire : seuils arbitraires à ajuster si besoin\n",
    "def categorize_ship_size(gt):\n",
    "    if pd.isna(gt):\n",
    "        return \"Inconnu\"\n",
    "    elif gt < 3000:\n",
    "        return \"Petit\"\n",
    "    elif gt < 15000:\n",
    "        return \"Moyen\"\n",
    "    else:\n",
    "        return \"Grand\"\n",
    "\n",
    "df[\"Ship_Profile_Class\"] = df[\"Sh1Size_gt_clean\"].apply(categorize_ship_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation de l'heure (Time) pour catégorisation\n",
    "def classify_time_of_day(time_str):\n",
    "    try:\n",
    "        # Conversion vers objet time\n",
    "        t = datetime.datetime.strptime(time_str, \"%I:%M:%S %p\").time()\n",
    "        if t >= datetime.time(6, 0) and t < datetime.time(12, 0):\n",
    "            return \"Matin\"\n",
    "        elif t >= datetime.time(12, 0) and t < datetime.time(18, 0):\n",
    "            return \"Après-midi\"\n",
    "        elif t >= datetime.time(18, 0) and t < datetime.time(22, 0):\n",
    "            return \"Soir\"\n",
    "        else:\n",
    "            return \"Nuit\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Application sur la colonne Time\n",
    "df[\"Time_Period\"] = df[\"Time\"].apply(classify_time_of_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d45e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catégorisation géographique simple\n",
    "def categorize_latitude(lat):\n",
    "    if lat < 56:\n",
    "        return \"Sud\"\n",
    "    elif 56 <= lat < 59:\n",
    "        return \"Centre\"\n",
    "    else:\n",
    "        return \"Nord\"\n",
    "\n",
    "def categorize_longitude(lon):\n",
    "    if lon < 12:\n",
    "        return \"Ouest\"\n",
    "    elif 12 <= lon < 18:\n",
    "        return \"Centre\"\n",
    "    else:\n",
    "        return \"Est\"\n",
    "\n",
    "df[\"Geo_Latitude_Zone\"] = df[\"Latitude\"].apply(categorize_latitude)\n",
    "df[\"Geo_Longitude_Zone\"] = df[\"Longitude\"].apply(categorize_longitude)\n",
    "df[\"Geo_Zone\"] = df[\"Geo_Latitude_Zone\"] + \"-\" + df[\"Geo_Longitude_Zone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformisation de Pollution_Qualité\n",
    "df[\"Pollution_Qualité\"] = df[\"Pollution_Qualité\"].str.strip().str.capitalize()\n",
    "df[\"Pollution_Qualité\"] = df[\"Pollution_Qualité\"].replace({\n",
    "    \"Partiel\": \"Partiel\",\n",
    "    \"Partielle\": \"Partiel\",\n",
    "    \"Fiable\": \"Fiable\",\n",
    "    \"Reliable\": \"Fiable\",\n",
    "    \"Non renseigné\": None,\n",
    "    \"Non renseignée\": None,\n",
    "    \"\": None,\n",
    "    None: None\n",
    "})\n",
    "\n",
    "# Uniformisation de Time_Period\n",
    "df[\"Time_Period\"] = df[\"Time_Period\"].str.strip().str.capitalize()\n",
    "df[\"Time_Period\"] = df[\"Time_Period\"].replace({\n",
    "    \"Matin\": \"Matin\",\n",
    "    \"Morning\": \"Matin\",\n",
    "    \"Après-midi\": \"Après-midi\",\n",
    "    \"Apres-midi\": \"Après-midi\",\n",
    "    \"Après Midi\": \"Après-midi\",\n",
    "    \"Soir\": \"Soir\",\n",
    "    \"Evening\": \"Soir\",\n",
    "    \"Nuit\": \"Nuit\",\n",
    "    \"Night\": \"Nuit\",\n",
    "    \"\": None,\n",
    "    None: None\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfe303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Étape 1 : Variables utilisées\n",
    "features = df[['Damage_Severe', 'Pollution_Score', 'Ship_Profile_Score']].copy()\n",
    "features = features.fillna(0)\n",
    "\n",
    "# Étape 2 : Normalisation robuste\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
    "\n",
    "# Étape 3 : Pondérations métier\n",
    "w_damage = 0.5\n",
    "w_pollution = 0.35\n",
    "w_ship = 0.15\n",
    "\n",
    "# Étape 4 : Calcul du Risk Score + amplification pour lisibilité\n",
    "df['Risk_Score'] = (\n",
    "    w_damage * features_scaled['Damage_Severe'] +\n",
    "    w_pollution * features_scaled['Pollution_Score'] +\n",
    "    w_ship * features_scaled['Ship_Profile_Score']\n",
    ")\n",
    "\n",
    "# multiplier pour sortir du 0.0\n",
    "df['Risk_Score'] *= 10\n",
    "\n",
    "# Étape 5 : Classification par seuils métiers\n",
    "def risk_classification(score):\n",
    "    if score < 1:\n",
    "        return 'Bas'\n",
    "    elif score < 3:\n",
    "        return 'Medium'\n",
    "    elif score < 6:\n",
    "        return 'Haut'\n",
    "    else:\n",
    "        return 'Critique'\n",
    "\n",
    "df['Risk_Class'] = df['Risk_Score'].apply(risk_classification)\n",
    "\n",
    "# Vérif console\n",
    "print(df['Risk_Class'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6074c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 2 : Classification équilibrée par quartiles\n",
    "q = df['Risk_Score'].quantile([0, 0.25, 0.5, 0.75, 1]).values\n",
    "\n",
    "def risk_classification_quantile(score):\n",
    "    if score <= q[1]:\n",
    "        return 'Bas'\n",
    "    elif score <= q[2]:\n",
    "        return 'Medium'\n",
    "    elif score <= q[3]:\n",
    "        return 'Haut'\n",
    "    else:\n",
    "        return 'Critique'\n",
    "\n",
    "df['Risk_Class'] = df['Risk_Score'].apply(risk_classification_quantile)\n",
    "\n",
    "# Vérif\n",
    "print(df['Risk_Class'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8f49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Low      : 0 ≤ score ≤ {q[1]:.3f}\")\n",
    "print(f\"Medium   : {q[1]:.3f} < score ≤ {q[2]:.3f}\")\n",
    "print(f\"High     : {q[2]:.3f} < score ≤ {q[3]:.3f}\")\n",
    "print(f\"Critical : {q[3]:.3f} < score ≤ {q[4]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Audit complet et structuré du DataFrame ---\n",
    "\n",
    "# 1. Taux de complétion par colonne (% de valeurs non manquantes)\n",
    "print(\"Taux de complétion par colonne (%) :\")\n",
    "completion = (1 - df.isnull().mean()) * 100\n",
    "print(completion.sort_values(ascending=True), \"\\n\")\n",
    "\n",
    "# 2. Nombre de valeurs uniques par colonne (hors NaN)\n",
    "print(\"Colonnes avec des valeurs uniques :\")\n",
    "for col in df.columns:\n",
    "    uniques = df[col].dropna().unique()\n",
    "    print(f\"{col}: {len(uniques)} valeurs uniques\")\n",
    "print()\n",
    "\n",
    "# 3. Statistiques descriptives des colonnes numériques\n",
    "print(\"Statistiques sur les colonnes numériques :\")\n",
    "print(df.select_dtypes(include=[\"number\"]).describe().T, \"\\n\")\n",
    "\n",
    "# 4. Exemples de valeurs non numériques (max 5 exemples par colonne)\n",
    "print(\"Exemples de valeurs non numériques :\")\n",
    "for col in df.select_dtypes(exclude=[\"number\"]).columns:\n",
    "    exemples = df[col].dropna().unique()[:5]\n",
    "    print(f\"{col} → Exemples : {exemples}\")\n",
    "print()\n",
    "\n",
    "# 5. Répartition des classes pour la colonne 'Damage_Class'\n",
    "print(\"Répartition des classes Damage_Class :\")\n",
    "print(df[\"Damage_Class\"].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "# 6. Statistiques sur Pollution_Score\n",
    "print(\"Statistiques Pollution_Score :\")\n",
    "print(df[\"Pollution_Score\"].describe(), \"\\n\")\n",
    "\n",
    "# 7. Exemples de lignes où Pollution_Score > 0 (5 premiers)\n",
    "print(\"Exemples Pollution_Score > 0 :\")\n",
    "print(df.loc[df[\"Pollution_Score\"] > 0, [\"Pollu_t\", \"Pollu_m3\", \"Pollution_Score\"]].head(), \"\\n\")\n",
    "\n",
    "# 8. Valeurs uniques et répartition après uniformisation\n",
    "\n",
    "print(\"✅ Taux de complétion et classes uniques après uniformisation :\\n\")\n",
    "\n",
    "if \"Pollution_Qualité\" in df.columns:\n",
    "    print(\"Pollution_Qualité :\")\n",
    "    print(df[\"Pollution_Qualité\"].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "if \"Time_Period\" in df.columns:\n",
    "    print(\"Time_Period :\")\n",
    "    print(df[\"Time_Period\"].value_counts(dropna=False), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612946fe",
   "metadata": {},
   "source": [
    "## Export du dataset préparé pour le dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f511a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_a_garder = [\n",
    "    \"Unique_ID\",\n",
    "    \"Year\", \"Location\", \"Geo_Zone\", \"Geo_Latitude_Zone\", \"Geo_Longitude_Zone\",\n",
    "    \"Damage_Class\", \"Damage_Severe\", \"Pollution_Score\", \"Pollution_Qualité\", \"Risk_Score\", \"Risk_Class\",\n",
    "    \"Ship_Profile_Score\", \"Ship_Profile_Class\", \"Sh1Size_gt_clean\",\n",
    "    \"Time_Period\", \"Decade\",\n",
    "    \"Acc_Type\", \"Cargo_Type\", \"Colli_Type\", \"Assistance\",\n",
    "    \"Latitude\", \"Longitude\"\n",
    "]\n",
    "\n",
    "df_filtered = df[colonnes_a_garder]\n",
    "\n",
    "df_filtered.to_csv(\"../data/cleaned/shipping_accidents_cleaned.csv\", index=False)\n",
    "print(\"Fichier enregistré : ../data/cleaned/shipping_accidents_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
